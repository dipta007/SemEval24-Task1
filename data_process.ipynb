{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa262f2-828d-4c05-b718-57e1a1fc57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "RANDOM_SEED=42\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82d5890-702e-4a1e-b02e-547423f7b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_codes = {\n",
    "    \"amh\": \"amh_Ethi\",\n",
    "    \"ary\": \"ary_Arab\",\n",
    "    \"eng\": \"eng_Latn\",\n",
    "    \"esp\": \"spa_Latn\",\n",
    "    \"hau\": \"hau_Latn\",\n",
    "    \"kin\": \"kin_Latn\",\n",
    "    \"mar\": \"mar_Deva\",\n",
    "    \"tel\": \"tel_Telu\"\n",
    "}\n",
    "\n",
    "TRANSLATION_MODELS = [\n",
    "    (\"facebook/nllb-200-3.3B\", 8),\n",
    "    (\"facebook/nllb-200-1.3B\", 16),\n",
    "    (\"facebook/nllb-200-distilled-600M\", 16), # bsz fixed\n",
    "    (\"facebook/nllb-200-distilled-1.3B\", 16), # bsz fixed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7efdab49-5613-4205-99f7-10ee017115d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = [\n",
    "    # 'amh',\n",
    "    # 'arq',\n",
    "    'ary', 'eng', 'esp', 'hau', 'kin', 'mar', 'tel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207433d-c955-4099-9349-4b40d05e37b8",
   "metadata": {},
   "source": [
    "# Make train, val, dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090f18a-ee91-413d-afa3-3337e4953e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_train = 0\n",
    "tot_val = 0\n",
    "\n",
    "for lang in LANGS:\n",
    "    # if lang in [\"amh\", \"arq\"]: continue\n",
    "    if not os.path.isdir(f\"./data/Track A/{lang}\"): continue\n",
    "\n",
    "    df = pd.read_csv(f\"./data/Track A/{lang}/{lang}_train.csv\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "    train_df.to_csv(f\"./data/Track A/{lang}/{lang}_train_split.csv\")\n",
    "    val_df.to_csv(f\"./data/Track A/{lang}/{lang}_val_split.csv\")\n",
    "\n",
    "    def write_translation(mode):\n",
    "        file_name = f\"{lang}_{mode}\"\n",
    "        if mode not in [\"dev\", \"test\"]: file_name += '_split'\n",
    "            \n",
    "        df = pd.read_csv(f\"./data/Track A/{lang}/{file_name}.csv\")\n",
    "        df[\"text1\"] = df[\"Text\"].map(lambda x: x.split(\"\\n\")[0].strip('\"'))\n",
    "        df[\"text2\"] = df[\"Text\"].map(lambda x: x.split(\"\\n\")[1].strip('\"'))\n",
    "    \n",
    "        print(lang, mode, len(df))\n",
    "    \n",
    "        all_translations = defaultdict(list)\n",
    "        for tmodel_name, batch_size in tqdm(TRANSLATION_MODELS):\n",
    "            tmodel = AutoModelForSeq2SeqLM.from_pretrained(tmodel_name)\n",
    "            ttokenizer = AutoTokenizer.from_pretrained(tmodel_name)\n",
    "            source = lang_codes[lang]\n",
    "            target = \"eng_Latn\"\n",
    "            task_name = 'translation'\n",
    "            # if tmodel_name.index(\"mbart\") != -1: task_name = \"translation_te_to_en\"\n",
    "            translator = pipeline(task_name, model=tmodel, tokenizer=ttokenizer, src_lang=source, tgt_lang=target, batch_size=batch_size, device=DEVICE)\n",
    "        \n",
    "            texts1 = []\n",
    "            texts2 = []\n",
    "            for i, row in df.iterrows():\n",
    "                text1 = row['text1']\n",
    "                text2 = row['text2']\n",
    "                texts1.append(text1)\n",
    "                texts2.append(text2)\n",
    "            translations1 = translator(texts1, max_length=800)\n",
    "            translations1 = [x['translation_text'] for x in translations1]\n",
    "    \n",
    "            translations2 = translator(texts2, max_length=800)\n",
    "            translations2 = [x['translation_text'] for x in translations2]\n",
    "\n",
    "            for i, (_, row) in enumerate(df.iterrows()):\n",
    "                all_translations['text1'].append(translations1[i])\n",
    "                all_translations['text2'].append(translations2[i])\n",
    "                all_translations['PairID'].append(row['PairID'])\n",
    "                all_translations['model'].append(tmodel_name)\n",
    "                if mode != 'dev':\n",
    "                    all_translations['Score'].append(row['Score'])\n",
    "\n",
    "            if lang == 'eng': break\n",
    "    \n",
    "        out_df = pd.DataFrame(all_translations)\n",
    "        out_df.to_csv(f\"./data/Track A/{lang}/{mode}_translation.csv\")\n",
    "\n",
    "    for mode in ['train', 'val', 'dev']:\n",
    "        write_translation(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a78fa2-a358-4797-aee1-985292a2b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains = []\n",
    "all_vals = []\n",
    "all_devs = []\n",
    "\n",
    "for d in dirs:\n",
    "    df = pd.read_csv(f\"./data/Track A/{d}/train_translation.csv\")\n",
    "    df['lang'] = d\n",
    "    all_trains.append(df)\n",
    "\n",
    "    df = pd.read_csv(f\"./data/Track A/{d}/val_translation.csv\")\n",
    "    df['lang'] = d\n",
    "    all_vals.append(df)\n",
    "\n",
    "    df = pd.read_csv(f\"./data/Track A/{d}/dev_translation.csv\")\n",
    "    df['lang'] = d\n",
    "    all_devs.append(df)\n",
    "\n",
    "train = pd.concat(all_trains)\n",
    "val = pd.concat(all_vals)\n",
    "dev = pd.concat(all_devs)\n",
    "\n",
    "train.to_csv(\"./data/Track A/train_all.csv\")\n",
    "val.to_csv(\"./data/Track A/val_all.csv\")\n",
    "dev.to_csv(\"./data/Track A/dev_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06546a3a-750b-4a8e-9490-88ee341a0ef3",
   "metadata": {},
   "source": [
    "# Make test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45465c58-c2ca-4950-a5a1-06a56fbad34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATION_MODELS = [\n",
    "    (\"facebook/nllb-200-3.3B\", 8),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642b63e6-1418-48cc-a7c9-693ac653c469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ary test 426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aeb76b50284be48ec7a063786fd451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db137897d4645248718e11d5a4691fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng test 2600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5f4f9baa134f54aec869de823a7cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338d473fa86c4770bcd1f5e840c6ebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esp test 600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92cbf273948454694eab7614d7d5daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9ad02e09404dc5adda99ac9271067a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hau test 603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e2c21724f743dbb6030f758f0cc0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c106f722e948c39a0c5ee0f987676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kin test 222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8faba6feb2c4977b8ab046f1a1395fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be904da95bea420a8bd66893184ac8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mar test 298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9b6853e7bf4e308d31be33dab1f127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeb746f876d4373aac17943896c5fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tel test 297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a150b6ae4f494766b55975a889164c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cf1351c16d4feab754b9232f24c1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lang in LANGS:\n",
    "    if not os.path.isdir(f\"./test_data/Track A/{lang}\"): continue\n",
    "\n",
    "    def write_translation(mode):\n",
    "        file_name = f\"{lang}_{mode}\"\n",
    "            \n",
    "        df = pd.read_csv(f\"./test_data/Track A/{lang}/{file_name}.csv\")\n",
    "        df[\"text1\"] = df[\"Text\"].map(lambda x: x.split(\"\\n\")[0].strip('\"'))\n",
    "        df[\"text2\"] = df[\"Text\"].map(lambda x: x.split(\"\\n\")[1].strip('\"'))\n",
    "    \n",
    "        print(lang, mode, len(df))\n",
    "    \n",
    "        all_translations = defaultdict(list)\n",
    "        for tmodel_name, batch_size in tqdm(TRANSLATION_MODELS):\n",
    "            tmodel = AutoModelForSeq2SeqLM.from_pretrained(tmodel_name)\n",
    "            ttokenizer = AutoTokenizer.from_pretrained(tmodel_name)\n",
    "            source = lang_codes[lang]\n",
    "            target = \"eng_Latn\"\n",
    "            task_name = 'translation'\n",
    "            \n",
    "            translator = pipeline(task_name, model=tmodel, tokenizer=ttokenizer, src_lang=source, tgt_lang=target, batch_size=batch_size, device=DEVICE)\n",
    "        \n",
    "            texts1 = []\n",
    "            texts2 = []\n",
    "            for i, row in df.iterrows():\n",
    "                text1 = row['text1']\n",
    "                text2 = row['text2']\n",
    "                texts1.append(text1)\n",
    "                texts2.append(text2)\n",
    "            translations1 = translator(texts1, max_length=800)\n",
    "            translations1 = [x['translation_text'] for x in translations1]\n",
    "    \n",
    "            translations2 = translator(texts2, max_length=800)\n",
    "            translations2 = [x['translation_text'] for x in translations2]\n",
    "\n",
    "            for i, (_, row) in enumerate(df.iterrows()):\n",
    "                all_translations['text1'].append(translations1[i])\n",
    "                all_translations['text2'].append(translations2[i])\n",
    "                all_translations['PairID'].append(row['PairID'])\n",
    "                all_translations['model'].append(tmodel_name)\n",
    "\n",
    "            if lang == 'eng': break\n",
    "    \n",
    "        out_df = pd.DataFrame(all_translations)\n",
    "        out_df.to_csv(f\"./test_data/Track A/{lang}/{mode}_translation.csv\")\n",
    "\n",
    "    for mode in ['test']:\n",
    "        write_translation(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b68d90ee-ad20-45cf-89e8-c564c18fd019",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tests = []\n",
    "\n",
    "for d in dirs:\n",
    "    df = pd.read_csv(f\"./test_data/Track A/{d}/test_translation.csv\")\n",
    "    df['lang'] = d\n",
    "    all_tests.append(df)\n",
    "    \n",
    "test = pd.concat(all_tests)\n",
    "\n",
    "train.to_csv(\"./test_data/Track A/test_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d53fb-0d3c-4dee-9e41-0be779bbc0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
