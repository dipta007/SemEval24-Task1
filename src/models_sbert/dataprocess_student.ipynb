{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sentence_transformers.util\n",
    "from opustools import OpusRead\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import csv\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "LANGS = [\n",
    "    # 'amh',\n",
    "    # 'arq',\n",
    "    \"ary\",\n",
    "    \"eng\",\n",
    "    \"esp\",\n",
    "    \"hau\",\n",
    "    \"kin\",\n",
    "    \"mar\",\n",
    "    \"tel\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    if \"text1\" not in df.columns:\n",
    "        df[\"text1\"] = df[\"Text\"].map(lambda x: x.split(\"\\n\")[0].strip('\"'))\n",
    "    if \"text2\" not in df.columns:\n",
    "        df[\"text2\"] = df[\"Text\"].map(lambda x: x.split(\"\\n\")[1].strip('\"'))\n",
    "    df[\"score\"] = df[\"Score\"].map(lambda x: float(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(lang):\n",
    "    train_df = pd.read_csv(f\"../../data/Track A/{lang}/train_translation.csv\")\n",
    "    train_df['PairID'] = train_df['PairID'].astype(str)\n",
    "\n",
    "    org_train_df = pd.read_csv(f\"../../data/Track A/{lang}/{lang}_train.csv\")\n",
    "    org_train_df = preprocess_df(org_train_df)\n",
    "    org_train_df['PairID'] = org_train_df['PairID'].astype(str)\n",
    "\n",
    "    combined = train_df.merge(org_train_df, on=\"PairID\", how=\"left\")\n",
    "    combined['lang'] = lang\n",
    "    combined = combined[['PairID', 'text1_x', 'text2_x', 'text1_y', 'text2_y', 'Score_x', 'lang', 'model']]\n",
    "\n",
    "    combined.rename(columns={'text1_x': 't1_src', 'text2_x': 't2_src', 'text1_y': 't1_tgt', 'text2_y': 't2_tgt', 'Score_x': 'score'}, inplace=True)\n",
    "\n",
    "    ids = list(set(combined['PairID'].values))\n",
    "    train_ids, val_ids = train_test_split(ids, test_size=0.05, random_state=RANDOM_SEED)\n",
    "\n",
    "    train_df = combined[combined['PairID'].isin(train_ids)]\n",
    "    val_df = combined[combined['PairID'].isin(val_ids)]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['src'] = pd.concat([train_df['t1_src'], train_df['t2_src']])\n",
    "    df['tgt'] = pd.concat([train_df['t1_tgt'], train_df['t2_tgt']])\n",
    "\n",
    "    return train_df, val_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ary\n",
      "(3156, 8) (168, 8) 42 (6312, 2)\n",
      "eng\n",
      "(4702, 8) (248, 8) 248 (9404, 2)\n",
      "esp\n",
      "(5336, 8) (284, 8) 71 (10672, 2)\n",
      "hau\n",
      "(5932, 8) (316, 8) 79 (11864, 2)\n",
      "kin\n",
      "(2660, 8) (140, 8) 35 (5320, 2)\n",
      "mar\n",
      "(4104, 8) (216, 8) 54 (8208, 2)\n",
      "tel\n",
      "(4000, 8) (212, 8) 53 (8000, 2)\n"
     ]
    }
   ],
   "source": [
    "for lang in LANGS:\n",
    "    train_df, val_df, df = data_process(lang)\n",
    "\n",
    "    df.to_csv(f\"../../data/Track A/{lang}/train_student.tsv\", sep=\"\\t\", index=False)\n",
    "    val_df.to_csv(f\"../../data/Track A/{lang}/val_student.tsv\", sep=\"\\t\", index=False)\n",
    "    print(lang)\n",
    "    print(train_df.shape, val_df.shape, len(set(val_df['PairID'].values)), df.shape)\n",
    "    all_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>t1_src</th>\n",
       "      <th>t2_src</th>\n",
       "      <th>t1_tgt</th>\n",
       "      <th>t2_tgt</th>\n",
       "      <th>score</th>\n",
       "      <th>lang</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TEL-train-00589</td>\n",
       "      <td>The film was a box office success.</td>\n",
       "      <td>Bollywood actor Ajay Devgn is also planning to...</td>\n",
       "      <td>ఈ సినిమా బాలీవుడ్ లో కలెక్షన్స్ బాగా తెచ్చుకుంది.</td>\n",
       "      <td>అంతేకాక ఈ షెడ్యూల్లోనే బాలీవుడ్ నటుడు అజయ్ దేవ...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>tel</td>\n",
       "      <td>facebook/nllb-200-1.3B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TEL-train-00232</td>\n",
       "      <td>Jaycee Divakarreddhi has made harsh comments o...</td>\n",
       "      <td>Jaycee Divakarreddhi, angry at the VCP governm...</td>\n",
       "      <td>రాష్ట్రంలో కొందరిని లక్ష్యంగా చేసుకుని కేసులు ...</td>\n",
       "      <td>వైసీపీ ప్రభుత్వంపై జేసీ దివాకర్రెడ్డి మండిపడుత...</td>\n",
       "      <td>0.65</td>\n",
       "      <td>tel</td>\n",
       "      <td>facebook/nllb-200-1.3B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TEL-train-01000</td>\n",
       "      <td>There are reports that theatres will be closed...</td>\n",
       "      <td>You are the reason why I am so happy, so my jo...</td>\n",
       "      <td>ఏప్రిల్ 15 తర్వాత మళ్లీ థియేటర్ల బంద్ లేదా 50%...</td>\n",
       "      <td>నేను ఇంత ఆనందంగా ఉండటానికి కారణం నువ్వే. . అంద...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>tel</td>\n",
       "      <td>facebook/nllb-200-1.3B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>TEL-train-00979</td>\n",
       "      <td>But Shraddha Kapoor got good marks.</td>\n",
       "      <td>The Shiv Sena has decided to file another peti...</td>\n",
       "      <td>కానీ శ్రధ్ధా కపూర్ నటకు మాత్రం మంచి మార్కులు ప...</td>\n",
       "      <td>రాష్టప్రతి పాలనకు గవర్నర్ సిఫార్సు చేయడం, కేంద...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>tel</td>\n",
       "      <td>facebook/nllb-200-1.3B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>TEL-train-00068</td>\n",
       "      <td>Too much fun took the life of a young man.</td>\n",
       "      <td>The young man died from too much fun.</td>\n",
       "      <td>మితిమీరిన సరదా ఓ యువకుడి ప్రాణం తీసింది .</td>\n",
       "      <td>అతి సరదా వల్ల యువకుడు చనిపోయాడు.</td>\n",
       "      <td>0.75</td>\n",
       "      <td>tel</td>\n",
       "      <td>facebook/nllb-200-1.3B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PairID                                             t1_src  \\\n",
       "15   TEL-train-00589                 The film was a box office success.   \n",
       "36   TEL-train-00232  Jaycee Divakarreddhi has made harsh comments o...   \n",
       "37   TEL-train-01000  There are reports that theatres will be closed...   \n",
       "159  TEL-train-00979                But Shraddha Kapoor got good marks.   \n",
       "169  TEL-train-00068         Too much fun took the life of a young man.   \n",
       "\n",
       "                                                t2_src  \\\n",
       "15   Bollywood actor Ajay Devgn is also planning to...   \n",
       "36   Jaycee Divakarreddhi, angry at the VCP governm...   \n",
       "37   You are the reason why I am so happy, so my jo...   \n",
       "159  The Shiv Sena has decided to file another peti...   \n",
       "169              The young man died from too much fun.   \n",
       "\n",
       "                                                t1_tgt  \\\n",
       "15   ఈ సినిమా బాలీవుడ్ లో కలెక్షన్స్ బాగా తెచ్చుకుంది.   \n",
       "36   రాష్ట్రంలో కొందరిని లక్ష్యంగా చేసుకుని కేసులు ...   \n",
       "37   ఏప్రిల్ 15 తర్వాత మళ్లీ థియేటర్ల బంద్ లేదా 50%...   \n",
       "159  కానీ శ్రధ్ధా కపూర్ నటకు మాత్రం మంచి మార్కులు ప...   \n",
       "169          మితిమీరిన సరదా ఓ యువకుడి ప్రాణం తీసింది .   \n",
       "\n",
       "                                                t2_tgt  score lang  \\\n",
       "15   అంతేకాక ఈ షెడ్యూల్లోనే బాలీవుడ్ నటుడు అజయ్ దేవ...   0.47  tel   \n",
       "36   వైసీపీ ప్రభుత్వంపై జేసీ దివాకర్రెడ్డి మండిపడుత...   0.65  tel   \n",
       "37   నేను ఇంత ఆనందంగా ఉండటానికి కారణం నువ్వే. . అంద...   0.32  tel   \n",
       "159  రాష్టప్రతి పాలనకు గవర్నర్ సిఫార్సు చేయడం, కేంద...   0.34  tel   \n",
       "169                   అతి సరదా వల్ల యువకుడు చనిపోయాడు.   0.75  tel   \n",
       "\n",
       "                      model  \n",
       "15   facebook/nllb-200-1.3B  \n",
       "36   facebook/nllb-200-1.3B  \n",
       "37   facebook/nllb-200-1.3B  \n",
       "159  facebook/nllb-200-1.3B  \n",
       "169  facebook/nllb-200-1.3B  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Producer K.K. Radhamohan is preparing to relea...</td>\n",
       "      <td>అన్ని కార్యక్రమాలు పూర్తిచేసి ఈ చిత్రాన్ని ఈనె...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Speaking on the occasion, Zonal CPM Secretary ...</td>\n",
       "      <td>ఈ సందర్భంగా మండల సిపిఎం కార్యదర్శి వెల్లంపల్లి...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The police have registered a case and are inve...</td>\n",
       "      <td>ఈ మేరకు కేసు నమోదు చేసుకున్న పోలీసులు దర్యాప్త...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A cargo plane crashed into the sea off the Ivo...</td>\n",
       "      <td>సరుకులు తీసుకువెళ్తున్న ఓ కార్గో విమానం సముద్ర...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With this, Windy scored 183 runs for the loss ...</td>\n",
       "      <td>దీంతో విండీస్ 18.1 ఓవర్లలో 4 వికెట్ల నష్టానికి...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  Producer K.K. Radhamohan is preparing to relea...   \n",
       "1  Speaking on the occasion, Zonal CPM Secretary ...   \n",
       "2  The police have registered a case and are inve...   \n",
       "3  A cargo plane crashed into the sea off the Ivo...   \n",
       "4  With this, Windy scored 183 runs for the loss ...   \n",
       "\n",
       "                                                 tgt  \n",
       "0  అన్ని కార్యక్రమాలు పూర్తిచేసి ఈ చిత్రాన్ని ఈనె...  \n",
       "1  ఈ సందర్భంగా మండల సిపిఎం కార్యదర్శి వెల్లంపల్లి...  \n",
       "2  ఈ మేరకు కేసు నమోదు చేసుకున్న పోలీసులు దర్యాప్త...  \n",
       "3  సరుకులు తీసుకువెళ్తున్న ఓ కార్గో విమానం సముద్ర...  \n",
       "4  దీంతో విండీస్ 18.1 ఓవర్లలో 4 వికెట్ల నష్టానికి...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['tgt'].map(lambda x: len(x.split())).hist(bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_languages = set([\"en\"])  # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(\n",
    "    # all needed: \n",
    "    ['arq', 'amh', 'am', 'ha', 'hau', 'kin', 'rw', 'mr', 'ary', 'es', 'te']\n",
    "    # needed: ['arq', 'am', 'ha', 'rw', 'mr', 'ary', 'es', 'te']\n",
    "    # available:\n",
    "    # ['arq', 'am', 'ha', 'mr', 'es', 'te']\n",
    "    # [\"de\", \"es\", \"it\", \"fr\", \"ar\", \"tr\"]\n",
    ")  # We want to extend the model to these new languages. For language codes, see the header of the train file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Talks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel-sentences.tsv.gz does not exists. Try to download from server\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadbe03eda764372a6238b7feab867a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/581M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel sentences files en-rw, en-kin, en-hau, en-ary, en-amh do not exist. Create these files now\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0133cf517a24233ac331ff44305bc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'en', 'trg_lang': 'rw', 'fTrain': '../../data/student/talks/talks-en-rw-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-rw-dev.tsv.gz', 'devCount': 0} 'rw'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb1dd9987124598b35234c1370f5eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'en', 'trg_lang': 'kin', 'fTrain': '../../data/student/talks/talks-en-kin-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-kin-dev.tsv.gz', 'devCount': 0} 'kin'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1830048aede4cd29f42fd133d815b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'en', 'trg_lang': 'hau', 'fTrain': '../../data/student/talks/talks-en-hau-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-hau-dev.tsv.gz', 'devCount': 0} 'hau'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b08ad46a89495da00ee40b063ef450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'en', 'trg_lang': 'ary', 'fTrain': '../../data/student/talks/talks-en-ary-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-ary-dev.tsv.gz', 'devCount': 0} 'ary'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff58e47b82243848b927359d81d3dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'en', 'trg_lang': 'amh', 'fTrain': '../../data/student/talks/talks-en-amh-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-amh-dev.tsv.gz', 'devCount': 0} 'amh'\n",
      "{'src_lang': 'en', 'trg_lang': 'rw', 'fTrain': '../../data/student/talks/talks-en-rw-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-rw-dev.tsv.gz', 'devCount': 0} 'str' object has no attribute 'close'\n",
      "{'src_lang': 'en', 'trg_lang': 'kin', 'fTrain': '../../data/student/talks/talks-en-kin-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-kin-dev.tsv.gz', 'devCount': 0} 'str' object has no attribute 'close'\n",
      "{'src_lang': 'en', 'trg_lang': 'hau', 'fTrain': '../../data/student/talks/talks-en-hau-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-hau-dev.tsv.gz', 'devCount': 0} 'str' object has no attribute 'close'\n",
      "{'src_lang': 'en', 'trg_lang': 'ary', 'fTrain': '../../data/student/talks/talks-en-ary-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-ary-dev.tsv.gz', 'devCount': 0} 'str' object has no attribute 'close'\n",
      "{'src_lang': 'en', 'trg_lang': 'amh', 'fTrain': '../../data/student/talks/talks-en-amh-train.tsv.gz', 'fDev': '../../data/student/talks/talks-en-amh-dev.tsv.gz', 'devCount': 0} 'str' object has no attribute 'close'\n",
      "---DONE---\n"
     ]
    }
   ],
   "source": [
    "dev_sentences = 1000  # Number of sentences we want to use for development\n",
    "download_url = \"https://sbert.net/datasets/parallel-sentences.tsv.gz\"  # Specify parallel sentences URL here\n",
    "\n",
    "parallel_sentences_path = \"./talks/talks.tsv.gz\"  # Path of the parallel-sentences.tsv.gz file.\n",
    "parallel_sentences_folder = \"../../data/student/talks/\"\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(parallel_sentences_path), exist_ok=True)\n",
    "if not os.path.exists(parallel_sentences_path):\n",
    "    print(\"parallel-sentences.tsv.gz does not exists. Try to download from server\")\n",
    "    sentence_transformers.util.http_get(download_url, parallel_sentences_path)\n",
    "\n",
    "\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(\n",
    "            parallel_sentences_folder, \"talks-{}-{}-train.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        output_filename_dev = os.path.join(\n",
    "            parallel_sentences_folder, \"talks-{}-{}-dev.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append(\n",
    "                {\n",
    "                    \"src_lang\": source_lang,\n",
    "                    \"trg_lang\": target_lang,\n",
    "                    'fTrain': output_filename_train,\n",
    "                    'fDev': output_filename_dev,\n",
    "                    \"devCount\": 0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\n",
    "        \"Parallel sentences files {} do not exist. Create these files now\".format(\n",
    "            \", \".join(map(lambda x: x[\"src_lang\"] + \"-\" + x[\"trg_lang\"], files_to_create))\n",
    "        )\n",
    "    )\n",
    "    for outfile in files_to_create:\n",
    "        try:\n",
    "            with gzip.open(parallel_sentences_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "                reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "                flg = 0\n",
    "                for line in tqdm(reader, desc=\"Sentences\"):\n",
    "                    src_text = line[outfile[\"src_lang\"]].strip()\n",
    "                    trg_text = line[outfile[\"trg_lang\"]].strip()\n",
    "\n",
    "                    if flg == 0:\n",
    "                        outfile[\"fTrain\"] = gzip.open(outfile[\"fTrain\"], \"wt\", encoding=\"utf8\")\n",
    "                        outfile[\"fDev\"] = gzip.open(outfile[\"fDev\"], \"wt\", encoding=\"utf8\")\n",
    "                        flg = 1\n",
    "\n",
    "                    if src_text != \"\" and trg_text != \"\":\n",
    "                        if outfile[\"devCount\"] < dev_sentences:\n",
    "                            outfile[\"devCount\"] += 1\n",
    "                            fOut = outfile[\"fDev\"]\n",
    "                        else:\n",
    "                            fOut = outfile[\"fTrain\"]\n",
    "\n",
    "                        fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "        except Exception as e:\n",
    "            print(outfile, e)\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        try:\n",
    "            outfile[\"fTrain\"].close()\n",
    "            outfile[\"fDev\"].close()\n",
    "        except Exception as e:\n",
    "            print(outfile, e)\n",
    "\n",
    "shutil.rmtree(\"./talks/\")\n",
    "print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get WikiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.arq-en.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.arq-en.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.arq-en.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-rw.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-rw.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-rw.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-kin.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-kin.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-kin.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-hau.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-hau.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-hau.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.am-en.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.am-en.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.am-en.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ha.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ha.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ha.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.ary-en.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.ary-en.tsv.gz. Response 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.ary-en.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.amh-en.tsv.gz\n",
      "Was not able to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.amh-en.tsv.gz\n",
      "---DONE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.amh-en.tsv.gz. Response 403\n"
     ]
    }
   ],
   "source": [
    "num_dev_sentences = 1000  # Number of sentences we want to use for development\n",
    "threshold = 1.075  # Only use sentences with a LASER similarity score above the threshold\n",
    "\n",
    "download_url = \"https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/\"\n",
    "download_folder = \"./WikiMatrix/\"\n",
    "parallel_sentences_folder = \"../../data/student/WikiMatrix/\"\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(download_folder), exist_ok=True)\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        filename_train = os.path.join(\n",
    "            parallel_sentences_folder, \"WikiMatrix-{}-{}-train.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        filename_dev = os.path.join(\n",
    "            parallel_sentences_folder, \"WikiMatrix-{}-{}-dev.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(filename_train) and not os.path.exists(filename_dev):\n",
    "            langs_ordered = sorted([source_lang, target_lang])\n",
    "            wikimatrix_filename = \"WikiMatrix.{}-{}.tsv.gz\".format(*langs_ordered)\n",
    "            wikimatrix_filepath = os.path.join(download_folder, wikimatrix_filename)\n",
    "\n",
    "            if not os.path.exists(wikimatrix_filepath):\n",
    "                print(\"Download\", download_url + wikimatrix_filename)\n",
    "                try:\n",
    "                    sentence_transformers.util.http_get(download_url + wikimatrix_filename, wikimatrix_filepath)\n",
    "                except Exception:\n",
    "                    print(\"Was not able to download\", download_url + wikimatrix_filename)\n",
    "                    continue\n",
    "\n",
    "            if not os.path.exists(wikimatrix_filepath):\n",
    "                continue\n",
    "\n",
    "            train_sentences = []\n",
    "            dev_sentences = []\n",
    "            dev_sentences_set = set()\n",
    "            extract_dev_sentences = True\n",
    "\n",
    "            with gzip.open(wikimatrix_filepath, \"rt\", encoding=\"utf8\") as fIn:\n",
    "                for line in fIn:\n",
    "                    score, sent1, sent2 = line.strip().split(\"\\t\")\n",
    "                    sent1 = sent1.strip()\n",
    "                    sent2 = sent2.strip()\n",
    "                    score = float(score)\n",
    "\n",
    "                    if score < threshold:\n",
    "                        break\n",
    "\n",
    "                    if sent1 == sent2:\n",
    "                        continue\n",
    "\n",
    "                    if langs_ordered.index(source_lang) == 1:  # Swap, so that src lang is sent1\n",
    "                        sent1, sent2 = sent2, sent1\n",
    "\n",
    "                    # Avoid duplicates in development set\n",
    "                    if sent1 in dev_sentences_set or sent2 in dev_sentences_set:\n",
    "                        continue\n",
    "\n",
    "                    if extract_dev_sentences:\n",
    "                        dev_sentences.append([sent1, sent2])\n",
    "                        dev_sentences_set.add(sent1)\n",
    "                        dev_sentences_set.add(sent2)\n",
    "\n",
    "                        if len(dev_sentences) >= num_dev_sentences:\n",
    "                            extract_dev_sentences = False\n",
    "                    else:\n",
    "                        train_sentences.append([sent1, sent2])\n",
    "\n",
    "            print(\"Write\", len(dev_sentences), \"dev sentences\", filename_dev)\n",
    "            with gzip.open(filename_dev, \"wt\", encoding=\"utf8\") as fOut:\n",
    "                for sents in dev_sentences:\n",
    "                    fOut.write(\"\\t\".join(sents))\n",
    "                    fOut.write(\"\\n\")\n",
    "\n",
    "            print(\"Write\", len(train_sentences), \"train sentences\", filename_train)\n",
    "            with gzip.open(filename_train, \"wt\", encoding=\"utf8\") as fOut:\n",
    "                for sents in train_sentences:\n",
    "                    fOut.write(\"\\t\".join(sents))\n",
    "                    fOut.write(\"\\n\")\n",
    "\n",
    "\n",
    "shutil.rmtree(\"./WikiMatrix/\")\n",
    "print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download https://downloads.tatoeba.org/exports/sentences.tar.bz2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d847c8dbec48e0be807416bce0e04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/187M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download https://downloads.tatoeba.org/exports/links.tar.bz2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adf7145ba7f4f97b545f9d28a4e6a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/126M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract ./tatoeba/sentences.tar.bz2\n",
      "Extract ./tatoeba/links.tar.bz2\n",
      "Read sentences.csv file\n",
      "Read links.csv\n",
      "Write output files\n",
      "eng-tel has 280 sentences\n",
      "eng-kin has 19 sentences\n",
      "eng-hau has 21867 sentences\n",
      "eng-spa has 226989 sentences\n",
      "eng-amh has 194 sentences\n",
      "eng-mar has 48758 sentences\n",
      "---DONE---\n"
     ]
    }
   ],
   "source": [
    "# Note: Tatoeba uses 3 letter languages codes (ISO-639-2),\n",
    "# while other datasets like OPUS use 2 letter language codes (ISO-639-1)\n",
    "# For training of sentence transformers, which type of language code is used doesn't matter.\n",
    "# For language codes, see: https://en.wikipedia.org/wiki/List_of_ISO_639-2_codes\n",
    "source_languages_3 = set([\"eng\"])\n",
    "target_languages_3 = set([\"amh\", \"hau\", \"kin\", \"mar\", \"spa\", \"tel\"])\n",
    "\n",
    "num_dev_sentences = 1000  # Number of sentences that are used to create a development set\n",
    "\n",
    "\n",
    "tatoeba_folder = \"./tatoeba\"\n",
    "output_folder = \"../../data/student/tatoeba/\"\n",
    "\n",
    "\n",
    "sentences_file_bz2 = os.path.join(tatoeba_folder, \"sentences.tar.bz2\")\n",
    "sentences_file = os.path.join(tatoeba_folder, \"sentences.csv\")\n",
    "links_file_bz2 = os.path.join(tatoeba_folder, \"links.tar.bz2\")\n",
    "links_file = os.path.join(tatoeba_folder, \"links.csv\")\n",
    "\n",
    "download_url = \"https://downloads.tatoeba.org/exports/\"\n",
    "\n",
    "\n",
    "os.makedirs(tatoeba_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Download files if needed\n",
    "for filepath in [sentences_file_bz2, links_file_bz2]:\n",
    "    if not os.path.exists(filepath):\n",
    "        url = download_url + os.path.basename(filepath)\n",
    "        print(\"Download\", url)\n",
    "        sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "# Extract files if needed\n",
    "if not os.path.exists(sentences_file):\n",
    "    print(\"Extract\", sentences_file_bz2)\n",
    "    tar = tarfile.open(sentences_file_bz2, \"r:bz2\")\n",
    "    tar.extract(\"sentences.csv\", path=tatoeba_folder)\n",
    "    tar.close()\n",
    "\n",
    "if not os.path.exists(links_file):\n",
    "    print(\"Extract\", links_file_bz2)\n",
    "    tar = tarfile.open(links_file_bz2, \"r:bz2\")\n",
    "    tar.extract(\"links.csv\", path=tatoeba_folder)\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "# Read sentences\n",
    "sentences = {}\n",
    "all_langs = target_languages_3.union(source_languages_3)\n",
    "print(\"Read sentences.csv file\")\n",
    "with open(sentences_file, encoding=\"utf8\") as fIn:\n",
    "    for line in fIn:\n",
    "        id, lang, sentence = line.strip().split(\"\\t\")\n",
    "        if lang in all_langs:\n",
    "            sentences[id] = (lang, sentence)\n",
    "\n",
    "# Read links that map the translations between different languages\n",
    "print(\"Read links.csv\")\n",
    "translations = {src_lang: {trg_lang: {} for trg_lang in target_languages_3} for src_lang in source_languages_3}\n",
    "with open(links_file, encoding=\"utf8\") as fIn:\n",
    "    for line in fIn:\n",
    "        src_id, target_id = line.strip().split()\n",
    "\n",
    "        if src_id in sentences and target_id in sentences:\n",
    "            src_lang, src_sent = sentences[src_id]\n",
    "            trg_lang, trg_sent = sentences[target_id]\n",
    "\n",
    "            if src_lang in source_languages_3 and trg_lang in target_languages_3:\n",
    "                if src_sent not in translations[src_lang][trg_lang]:\n",
    "                    translations[src_lang][trg_lang][src_sent] = []\n",
    "                translations[src_lang][trg_lang][src_sent].append(trg_sent)\n",
    "\n",
    "# Write everything to the output folder\n",
    "print(\"Write output files\")\n",
    "for src_lang in source_languages_3:\n",
    "    for trg_lang in target_languages_3:\n",
    "        source_sentences = list(translations[src_lang][trg_lang])\n",
    "        train_sentences = source_sentences[num_dev_sentences:]\n",
    "        dev_sentences = source_sentences[0:num_dev_sentences]\n",
    "\n",
    "        print(\"{}-{} has {} sentences\".format(src_lang, trg_lang, len(source_sentences)))\n",
    "        if len(dev_sentences) > 0:\n",
    "            with gzip.open(\n",
    "                os.path.join(output_folder, \"Tatoeba-{}-{}-dev.tsv.gz\".format(src_lang, trg_lang)),\n",
    "                \"wt\",\n",
    "                encoding=\"utf8\",\n",
    "            ) as fOut:\n",
    "                for sent in dev_sentences:\n",
    "                    fOut.write(\"\\t\".join([sent] + translations[src_lang][trg_lang][sent]))\n",
    "                    fOut.write(\"\\n\")\n",
    "\n",
    "        if len(train_sentences) > 0:\n",
    "            with gzip.open(\n",
    "                os.path.join(output_folder, \"Tatoeba-{}-{}-train.tsv.gz\".format(src_lang, trg_lang)),\n",
    "                \"wt\",\n",
    "                encoding=\"utf8\",\n",
    "            ) as fOut:\n",
    "                for sent in train_sentences:\n",
    "                    fOut.write(\"\\t\".join([sent] + translations[src_lang][trg_lang][sent]))\n",
    "                    fOut.write(\"\\n\")\n",
    "\n",
    "shutil.rmtree(\"./tatoeba/\")\n",
    "print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OPUS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it through opus.py, as notebook hangs when running it\n",
    "\n",
    "# corpora = [\n",
    "#     \"NLLB\",\n",
    "#     \"QED\",\n",
    "#     \"TED2020 v1\"\n",
    "# ]  # Corpora you want to use\n",
    "\n",
    "# output_folder = \"../../data/student/\"\n",
    "# opus_download_folder = \"./opus\"\n",
    "\n",
    "# # Iterator over all corpora / source languages / target languages combinations and download files\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# for corpus in corpora:\n",
    "#     for src_lang in source_languages:\n",
    "#         for trg_lang in target_languages:\n",
    "#             corpus_folder = os.path.join(output_folder, corpus)\n",
    "#             os.makedirs(corpus_folder, exist_ok=True)\n",
    "#             output_filename = os.path.join(output_folder, corpus, \"OPUS-{}-{}-train.tsv.gz\".format(src_lang, trg_lang))\n",
    "#             if not os.path.exists(output_filename):\n",
    "#                 print(\"Create:\", output_filename)\n",
    "#                 try:\n",
    "#                     read = OpusRead(\n",
    "#                         directory=corpus,\n",
    "#                         source=src_lang,\n",
    "#                         target=trg_lang,\n",
    "#                         write=[output_filename],\n",
    "#                         download_dir=opus_download_folder,\n",
    "#                         preprocess=\"raw\",\n",
    "#                         write_mode=\"moses\",\n",
    "#                         suppress_prompts=True,\n",
    "#                         verbose=True,\n",
    "#                     )\n",
    "#                     read.printPairs()\n",
    "#                 except Exception as e:\n",
    "#                     print(\"error:\", output_filename, e)\n",
    "#                     os.remove(output_filename)\n",
    "\n",
    "# shutil.rmtree(\"./opus/\")\n",
    "# print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code_1toN = {\n",
    "    \"arq\": [\"arq\"],\n",
    "    \"am\": [\"am\", \"amh\"],\n",
    "    \"en\": [\"eng\", \"en\"],\n",
    "    \"ha\": [\"ha\", \"hau\"],\n",
    "    \"kin\": [\"rw\", \"kin\"],\n",
    "    \"mr\": [\"mr\"],\n",
    "    \"ary\": [\"ary\"],\n",
    "    \"es\": [\"es\", 'spa'],\n",
    "    \"te\": [\"te\"],\n",
    "}\n",
    "lang_code_Nto1 = {}\n",
    "for k, v in lang_code_1toN.items():\n",
    "    for v_ in v:\n",
    "        lang_code_Nto1[v_] = k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
